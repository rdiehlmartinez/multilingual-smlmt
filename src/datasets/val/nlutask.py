__author__ = "Richard Diehl Martinez"
""" Base interface class for NLU tasks (i.e. MLQA, XNLI, etc.)"""

import abc
import logging
import torch
import copy

from torch.optim import AdamW
from torch.utils.data import DataLoader, RandomSampler 

from ...taskheads import TaskHead

from ...utils import move_to_device

# import statements for type hints
from configparser import ConfigParser
from typing import List, Dict, Union, Tuple, Any
from torch import Tensor, device
from torch.utils.data import TensorDataset
from ...metalearners import BaseLearner
from ...models import BaseModel

logger = logging.getLogger(__name__)

class NLUTaskGenerator(metaclass=abc.ABCMeta):
    def __init__(self, config: ConfigParser, eval_type: str) -> None:
        """
        Base class for a NLU task (i.e. MLQA, XNLI, etc.). Each task generator stores parameters
        associated with the given task and provides an iterator that yields dicts of datasets
        for finetuning and evaluating the pre-trained model on the given task.
        """

        assert eval_type in [
            "standard",
            "few_shot",
            "cross_lingual",
        ], "eval_type must be one of standard, few-shot, cross-lingual"

        task_name = self.__class__.__name__.split("Generator")[0]
        self.config_name = task_name + "_" + eval_type.upper()

        self._batch_size = config.getint(self.config_name, "batch_size", fallback=128)
        self._max_epochs = config.getint(self.config_name, "max_epochs", fallback=5)
        self._lr = config.getfloat(self.config_name, "lr", fallback=1e-5)
        self._task_head_init_method = config.get(
            self.config_name, "task_head_init_method"
        )

        self._eval_type = eval_type  # few-shot, standard, cross-lingual

        # specify the languages to evaluate on
        self.eval_languages = config.get(self.config_name, "eval_languages").split(",")

    ### General properties of tasks ###

    @property
    def eval_type(self) -> str:
        """The type of evaluation that is being done (e.g. standard, few-shot, cross-lingual)"""
        return self._eval_type

    @property
    @abc.abstractmethod
    def task_type(self) -> str:
        """The type of NLU task (e.g. classifiation, qa)"""
        raise NotImplementedError

    ### Tasks define methods for computing a metric ###

    @property
    @abc.abstractmethod
    def metric_name(self) -> str:
        """The name of the metric used to evaluate the task"""
        raise NotImplementedError

    ### Tasks define methods for running evaluation of a given learner ### 

    @abc.abstractmethod
    def process_batch(self, data_batch, **kwargs) -> Dict[str, Tensor]:
        """ 
        Processes a batch of data. This function should take a batch of data as input and
        return a dictionary containing the inputs and labels for the task. 
        """
        raise NotImplementedError


    @abc.abstractmethod
    def run_evaluation(
        self, 
        finetune_model: BaseModel, 
        finetune_task_head_weights: Dict[str, torch.nn.Parameter],
        learner: BaseLearner,
        split_dataset: TensorDataset,
        device: device = None,
        **kwargs
    ): 
        """
        Runs evaluation of the model on the given task for a particular validation or test 
        split. Assumes that the model (along with the respective task weights) have already been
        finetuned. The model and optimization parameters are specified by the learner. 
        """
        raise NotImplementedError


    def run_finetune_evaluation(
        self,
        learner: BaseLearner, 
        task_data: Dict[str, Dict[str, Union[str, TensorDataset]]],
        device: device = None,
    ) -> Union[Tuple[float, float], Tuple[float, float, Dict[str, List[float]]]]:
        """ 
        Given some task_data that is generated by the __iter__ function of the task generator, 
        runs finetuning and evaluation of the model on this data. The model and optimization 
        parameters are specified by the learner.

        Returns:
        * eval_metric: A float indicating the evaluation metric on the eval_dataloader
        * eval_loss: A float indicating the loss on the eval_dataloader
        * finetune_info: A dictionary containing the losses and accuracies for the finetuning
            process
        """

        if device is None: 
            device = learner.base_device

        base_model = learner.base_model
        base_model.to(device)

        # Get the finetune and eval datasets
        finetune_dataset = task_data["finetune"]["dataset"]
        eval_dataset = task_data["eval"]["dataset"]

        # Setting up the task head for the task
        with torch.no_grad():

            if self.task_head_init_method == "protomaml":
                # If we are using protomaml, we use the first batch of the finetune dataset
                # to initialize the task head
                _finetune_sampler = RandomSampler(finetune_dataset)
                _finetune_dataloader = DataLoader(
                    finetune_dataset,
                    sampler=_finetune_sampler,
                    batch_size=self.batch_size,
                )

                init_data_batch = move_to_device(
                    self.process_batch(next(iter(_finetune_dataloader))),
                    device,
                )
            else:
                init_data_batch = None

            init_kwargs = self.get_eval_task_head_init_kwargs(
                learner,
                data_batch=init_data_batch,
                device=device,
            )

            task_head_weights = TaskHead.initialize_task_head(**init_kwargs)

        finetune_model = copy.deepcopy(base_model)

        # NOTE: we only train to convergence when we are training on the full dataset;
        # otherwise, we train for a fixed number of batches (i.e. epochs)
        train_to_convergence = self.eval_type != "few_shot"

        if train_to_convergence:
            assert (
                "dev" in task_data
            ), "Must provide a dev dataset when training to convergence"

            dev_dataset = task_data["dev"]["dataset"]

            patience = self.max_patience
            best_dev_metric = None

        # Setting up the optimizer
        finetune_optimizer_param_groups = learner.finetune_optimizer_param_groups(
            finetune_model,
            task_head_weights,
            add_decay_information=True,
            weight_decay_val=0.0,  # NOTE: un-tuned hyperparameter
        )
        finetune_optimizer = AdamW(
            finetune_optimizer_param_groups, lr=self.lr
        )
        finetune_model.train()

        total_step_num = 0
        early_exit_training = False

        return_finetune_info = self.eval_type == "standard"
        if return_finetune_info:
            # Setting up the training info dictionary
            finetune_info = []

        for epoch in range(self.max_epochs):

            if early_exit_training:
                break

            finetune_dataloader = DataLoader(
                finetune_dataset,
                sampler=RandomSampler(finetune_dataset),
                batch_size=self.batch_size,
            )

            # Finetune the model on the data in the finetune dataloader
            for finetune_batch in finetune_dataloader:

                finetune_optimizer.zero_grad()

                finetune_batch = self.process_batch(finetune_batch)
                finetune_batch = move_to_device(finetune_batch, device)

                outputs = finetune_model(
                    input_ids=finetune_batch["input_ids"],
                    attention_mask=finetune_batch["attention_mask"],
                )

                _, loss = learner._compute_task_loss(
                    outputs,
                    finetune_batch,
                    task_head_weights,
                    task_type=self.task_type,
                )

                # NOTE: Evaluate the model on the dev dataset every eval_every_n_steps if we are
                # training to convergence
                if (
                    train_to_convergence
                    and total_step_num % self.eval_every_n_steps == 0
                ):
                    finetune_model.eval()
                    dev_result = self.run_evaluation(
                        finetune_model,
                        task_head_weights,
                        learner,
                        dev_dataset,
                        device,
                        split_language=task_data["dev"]["language"],
                        split="dev"
                    )

                    if return_finetune_info: 
                        finetune_info.append({
                            "finetune_loss": loss.item(),
                            "dev_loss": dev_result["loss"],
                            "dev_metric": dev_result["metric"],
                            "step_num": total_step_num,
                        })

                    finetune_model.train()

                    if ( 
                        best_dev_metric is None 
                        or self.metric_is_better(dev_result["metric"], best_dev_metric)
                    ):
                        best_dev_metric = dev_result["metric"]
                        patience = self.max_patience
                    else:
                        patience -= 1
                        
                        if patience == 0:
                            early_exit_training = True

                if early_exit_training:
                    break                    

                # taking a finetune optimization step 
                loss.backward()
                finetune_optimizer.step()
                total_step_num += 1
            
        # NOTE: Model done training on the finetune dataset
        # now we evaluate on the final eval dataset

        finetune_model.eval()

        eval_result = self.run_evaluation(
            finetune_model,
            task_head_weights,
            learner,
            eval_dataset,
            device,
            split_language=task_data["eval"]["language"],
            split="eval"
        )

        eval_results = {
            "eval_loss": eval_result["loss"],
            "eval_metric": eval_result["metric"],
            "eval_language": task_data["eval"]["language"], 
            "num_finetune_steps": total_step_num,
        }

        if return_finetune_info:
            eval_results["finetune_info"] = finetune_info

        if self.eval_type == "cross_lingual":
            eval_results["finetune_language"] = task_data["finetune"]["language"]
        elif self.eval_type == "few_shot":
            eval_results["k"] = self.K
            eval_results["n"] = self.N

        return eval_results

    def get_eval_task_head_init_kwargs(
        self,
        learner: BaseLearner,
        data_batch: Dict[str, Tensor] = None,
        device: device = None,
    ) -> Dict[str, Any]:
        """
        Helper method for generating keyword arguments that can be passed into a task head
        initialization method to generate a task head for evaluation.
        NOTE: Should really only be called during evaluation

        Args:
            * learner: A BaseLearner object that stores the state (of the finetuned) model 
                along with the task head and helper methods for optimizing the model
            * data_batch: Batch of data used to initialize the task head if using
                the protomaml task_init_method
            * device: Device type used to initialize the task head with, if not
                specified defaults to self.base_device

        Returns:
            * init_kwargs (dict): Keyword arguments used by the task head initialization function
        """

        init_kwargs = {}

        init_kwargs["task_type"] = self.task_type
        init_kwargs["task_init_method"] = self.task_head_init_method

        if self.task_type == "classification":
            init_kwargs["n_labels"] = self.num_classes

        init_kwargs["base_model_hidden_dim"] = learner.base_model_hidden_dim
        init_kwargs["device"] = device if device is not None else learner.base_device

        if "protomaml" in self.task_head_init_method:
            assert (
                data_batch is not None
            ), "Use of protomaml as a classification head initializer requires a data_batch"

            assert(self.task_type == "classification"),\
                "Protomaml can only be used as a classification head initializer"

            init_kwargs["model"] = learner.base_model
            init_kwargs["data_batch"] = data_batch

        return init_kwargs

    @abc.abstractmethod
    def metric_is_better(self, curr_metric: float, best_metric: float) -> bool:
        """Returns True if curr_metric is 'better' than best_metric"""
        raise NotImplementedError
    
    ### A method for iterating over the task that yields dicts of datasets for each language ###

    @abc.abstractmethod
    def __iter__(self):
        """
        Should yield a dictionary structure storing the finetune, dev and eval datasets for the
        task for each language.

            ex. for cross-lingual evaluation english -> french
            [
                {
                    "finetune": {
                        "language": "en",
                        "dataset": torch.TensorDataset(...)
                    }
                    "dev": {
                        "language": "fr",
                        "dataset": torch.TensorDataset(...)
                    }
                    "eval": {
                        "language": "fr",
                        "dataset": torch.TensorDataset(...)
                    }
                },
                (... for other languages similar to above)
            ]
        """
        raise NotImplementedError()

    ### Properties for finetuning the model on this task ###

    @property
    def batch_size(self) -> int:
        """Batch size for finetuning the model on this task"""
        return self._batch_size

    @property
    def max_epochs(self) -> int:
        """Maximum number of epochs to finetune the model on this task"""
        return self._max_epochs

    @property
    def lr(self) -> float:
        """Learning rate for finetuning the model on this task"""
        return self._lr

    @property
    def task_head_init_method(self) -> str:
        """The method for initializing the task-specific head of the model"""
        return self._task_head_init_method

    ### Finetuning related properties ###
    @property
    def max_patience(self) -> int:
        return 3

    @property
    def eval_every_n_steps(self) -> int:
        return 30
