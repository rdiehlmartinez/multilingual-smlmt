# Trying to replicate the setup of Bansal et al.

[EXPERIMENT]
seed=42

# mode must either be train of inference
mode=train 
use_wandb=True
save_checkpoints=True

###### TRAINING DATASET CONFIGS ######

# configs for the meta dataset that stores datasets 
[META_DATASET]
root_path=../../rds-personal-3CBQLhZjXbU/data/oscar
languages=data/isocodes/xnli_languages.txt

task_sampling_method=proportional
task_sampling_prop_rate=0.3

# configs for each individual language dataset
[LANGUAGE_TASK]

# use meta-learning SMLMT label 
use_smlmt_labels=True

n=4
k=10
q=10

num_task_samples=8

sample_size=10_000
buffer_size=100_000_000

mask_sampling_method=proportional
mask_sampling_prop_rate=0.3
max_seq_len=128

###### MODEL ARCHITERTURE AND LEARNING ######

[BASE_MODEL]
name=xlm_r
# layers to be meta-learned 
trainable_layers=[0,1,2,3,4,5,6,7,8,9,10,11]

[LEARNER]
method=maml

# maml-specific configs
initial_base_model_lr=1e-4
initial_classifier_lr=1e-3
num_innerloop_steps=8
use_first_order=True

use_multiple_samples=True

# method for intializing the weights of the task classifier 
lm_head_init_method=protomaml
retain_lm_head=False

[PIPELINE]

meta_lr_scheduler_method=linear
meta_lr=1e-5

# can override the default device by specifying: 
# device=("cuda" or "cpu")

# number of tasks to sample before running global update step
num_tasks_per_iteration=4

# number of task batches before running evaluation 
# setting to 0 means we never evaluate
eval_every_n_iteration=100

# number of max task batches to train on
max_task_batch_steps=1500

# whether to run an eval loop before training
run_initial_eval=True

###### EVALUATION ######

[EVALUATION]
# for each evaluation task we need a corresponding dataset
# tasks should be comma-separated
standard_tasks=mlqa,standard
few_shot_tasks=xnli

[MLQA_STANDARD] 
train_data_dir=../../rds-personal-3CBQLhZjXbU/data/xtreme/download/squad
dev_data_dir=../../rds-personal-3CBQLhZjXbU/data/xtreme/download/mlqa/MLQA_V1/dev
eval_data_dir=../../rds-personal-3CBQLhZjXbU/data/xtreme/download/mlqa/MLQA_V1/test

task_head_init_method=random

batch_size=128
max_epochs=5
lr=1e-5

eval_languages=en

[XNLI_STANDARD]
data_dir=../../rds-personal-3CBQLhZjXbU/data/xtreme/download/xnli

task_head_init_method=random

batch_size=128
max_epochs=5
lr=1e-3

eval_languages=en


[XNLI_FEW_SHOT]
data_dir=../../rds-personal-3CBQLhZjXbU/data/xtreme/download/xnli

task_head_init_method=protomaml

batch_size=48
max_epochs=100
lr=1e-3

k=16

eval_languages=en


; [XNLI_CROSS_LINGUAL]
; data_dir=../../rds-personal-3CBQLhZjXbU/data/xtreme/download/xnli

; task_head_init_method=random

; batch_size=10
; max_epochs=5
; lr=1e-3

; eval_languages=fr


; [MLQA_CROSS_LINGUAL]
; train_data_dir=../../rds-personal-3CBQLhZjXbU/data/xtreme/download/squad
; dev_data_dir=../../rds-personal-3CBQLhZjXbU/data/xtreme/download/mlqa/MLQA_V1/dev
; eval_data_dir=../../rds-personal-3CBQLhZjXbU/data/xtreme/download/mlqa/MLQA_V1/test

; task_head_init_method=random

; batch_size=10
; max_epochs=5
; lr=1e-5

; eval_languages=de
