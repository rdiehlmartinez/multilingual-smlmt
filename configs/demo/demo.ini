[EXPERIMENT]
seed=42

# configs for the dataset that stores datasets 
[META_DATASET]
root_path=data/oscar
train_languages=data/cc100_languages.txt
# can add dev and test languages
task_sampling_method=proportional
task_sampling_prop_rate=0.3

# configs for each individual language dataset
[LANGUAGE_TASK_DATASET]
# CHANGE ME 
n=1
k=1
q=1
# CHANGE ME 
sample_size=1000 
mask_sampling_method=proportional
mask_sampling_prop_rate=0.3

[BASE_MODEL]
name=xlm_r
# layers to be meta-learned 
trainable_layers=[0,1,2,3,4,5,6,7,8,9,10,11]

[LEARNER]
method=platipus
optimizer_type=adam
meta_lr=0.01

# number of gradient updates in the inner loop
num_inner_steps=3
loss_function=cross_entropy
use_first_order=False

# how to represent a 'task embedding'
# using val_grad (the gradient on the validation set) is the method used 
# in the original platipus paper 
task_embedding_method=val_grad 

[PROBLYGLOT]
# number of tasks to sample before running global update step
num_tasks_per_iteration=10

# can override the default device by specifying device=("cuda" or "cpu")
