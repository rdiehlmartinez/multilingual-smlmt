# MAML few-shot training with N=25

[EXPERIMENT]
name=maml-fewshot
seed=42
use_wandb=False

###### TRAINING DATASET CONFIGS ######

# configs for the meta dataset that stores datasets 
[META_DATASET]
root_path=../../rds-personal-3CBQLhZjXbU/problyglot_data/oscar
languages=data/xnli_languages.txt

task_sampling_method=proportional
task_sampling_prop_rate=0.3

return_standard_labels=False

# configs for each individual language dataset
[LANGUAGE_TASK]
n=4
k=80
q=10

sample_size=10_000
buffer_size=100_000_000

mask_sampling_method=proportional
mask_sampling_prop_rate=0.3
max_seq_len=128

###### MODEL ARCHITERTURE AND LEARNING ######

[BASE_MODEL]
name=xlm_r
# layers to be meta-learned 
trainable_layers=[0,1,2,3,4,5,6,7,8,9,10,11]

[LEARNER]
method=maml

# to load in a checkpoint need to specify the file name and the desired saved run
; checkpoint_file=
; checkpoint_run=

# outer-loop learning parameters
optimizer_type=adam
meta_lr=1e-5

# inner-loop learning parameters
inner_lr=1e-3
classifier_lr=1e-3

num_learning_steps=7
use_multiple_samples=True
use_first_order=True

# method for intializing the weights of the task classifier 
lm_head_init_method=protomaml
retain_lm_head=False

[PROBLYGLOT]

# can override the default device by specifying: 
# device=("cuda" or "cpu")

# number of tasks to sample before running global update step
num_tasks_per_iteration=12

# number of task batches before running evaluation 
# setting to 0 means we never evaluate
eval_every_n_iteration=100

# number of max task batches to train on
max_task_batch_steps=1000

# whether to save the final model (after training)
save_final_model=False

# whether to run an eval loop before training
run_initial_eval=False

###### EVALUATION ######

[EVALUATION]
# for each evaluation task we need a corresponding dataset
# tasks should be comma-separated
tasks=xnli

# whether to run eval on dev or test
partition=dev
batch_size=512

# maximum number of batch steps before evaluation
# -1 indicates training on entire adaptation set
max_finetuning_batch_steps=10,50,100,-1

# whether to write checkpoints
save_checkpoints=False

[XNLI] 
root_path=../../rds-personal-3CBQLhZjXbU/problyglot_data/xtreme/download/xnli
task_head_init_method=random_fc