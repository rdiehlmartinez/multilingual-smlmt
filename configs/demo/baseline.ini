[EXPERIMENT]
name=baseline-zeroshot
seed=42
use_wandb=False

###### TRAINING DATASET CONFIGS ######

# configs for the meta dataset that stores datasets 
[META_DATASET]
root_path=data/oscar
languages=data/cc100_languages.txt

task_sampling_method=proportional
task_sampling_prop_rate=0.3

# configs for each individual language dataset
[LANGUAGE_TASK]
n=10
k=20
q=20

sample_size=10_000
buffer_size=100_000_000

mask_sampling_method=proportional
mask_sampling_prop_rate=0.3
max_seq_len=128

###### MODEL ARCHITERTURE AND LEARNING ######

[BASE_MODEL]
name=xlm_r
# layers to be meta-learned 
trainable_layers=[11]

[LEARNER]
method=baseline

# To load in a checkpoint need to specify the file name and the desired saved run
; checkpoint_file=
; checkpoint_run=

optimizer_type=adam

# method for intializing the weights of the task classifier 
lm_head_init_method=random

[PROBLYGLOT]

# can override the default device by specifying: 
# device=("cuda" or "cpu")

# number of tasks to sample before running global update step
num_tasks_per_iteration=20

# number of max task batches to train on
max_task_batch_steps=1000

# whether to save the final model (after training)
save_final_model=True