# Use for testing out configs quickly

[EXPERIMENT]
# change to platipus-zeroshot
name=platipus-demo
seed=42
use_wandb=False

###### TRAINING DATASET CONFIG ######

# configs for the meta dataset that stores datasets 
# NOTE: remove META_DATASET if no training is required
[META_DATASET]
root_path=data/oscar
languages=data/cc100_languages.txt

task_sampling_method=proportional
task_sampling_prop_rate=0.3

# specification for how to generate the N-way k-shot
# examples for each language
[LANGUAGE_TASK]
n=80
k=20
q=20

sample_size=10_000 
buffer_size=100_000_000
mask_sampling_method=proportional
mask_sampling_prop_rate=0.3
max_seq_len=128

###### MODEL ARCHITERTURE AND LEARNING ######

[BASE_MODEL]
name=xlm_r
# layers to be meta-learned 
trainable_layers=[11]

[LEARNER]
method=platipus

# To load in a checkpoint need to specify the file name and the desired saved run
; checkpoint_file=final.pt
; checkpoint_run=problyglot/platipus-zeroshot/9au0a3gy

optimizer_type=adam
meta_lr=0.01

# initial learning rates 
self.gamma_p=1e-2 
self.gamma_q=1e-2
self.inner_lr=1e-2
self.classifier_lr=0.1

kl_weight=1e-8

# number of gradient updates in the inner loop
num_inner_steps=5
use_first_order=True

# how to represent a 'task embedding'
# val_grad (the gradient on the validation set) is the method used 
# in the original platipus paper 
task_embedding_method=val_grad 

# method for intializing the weights of the lm head
lm_head_init_method=random_fc

[PROBLYGLOT]

# can override the default device by specifying: 
# device=("cuda" or "cpu")

# number of tasks to sample before running global update step
num_tasks_per_iteration=4

# number of task batches before running evaluation 
# setting to 0 means we never evaluate
eval_every_n_iteration=20

# number of max task batches to train on
max_task_batch_steps=2

# whether to save the final model (after training)
save_final_model=False

# whether to run an eval loop before training
run_initial_eval=False

###### EVALUATION ######

[EVALUATION]
# for each evaluation task we need a corresponding dataset
# tasks should be comma-separated
tasks=xnli

# whether to run eval on dev or test
partition=dev
batch_size=512

# maximum number of batch steps before evaluation
max_finetuning_batch_steps=10

# whether to write checkpoints
save_checkpoints=False

[XNLI]
root_path=data/xtreme/download/xnli
use_few_shot_adaptation=True
adapt_on_eval=False

# method for intializing the weights of the NLU task head
task_head_init_method=random_fc